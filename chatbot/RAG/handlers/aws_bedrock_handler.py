# ./chatbot/RAG/QA_AWS_Bedrock_Handler.py

import logging
from langchain_core.prompts import PromptTemplate
from chatbot.RAG.utils.singleton_meta import SingletonMeta
from chatbot.RAG.handlers.base_handler import BaseQAHandler
from chatbot.RAG.utils import utils
from ..clients.aws_client import get_client

logger = logging.getLogger(__name__)

class QA_AwsBedrockHandler(BaseQAHandler, metaclass=SingletonMeta):
    """
    Singleton class to handle interactions with the AWS Bedrock model
    for generating responses based on documents retrieved using TF-IDF.
    """

    def __init__(self, model: str, temperature: float, max_tokens: int, docs_directory: str,
                 chunk_size: int = 500, chunk_overlap: int = 0):
        """
        Initializes the handler with model parameters, prompt template, and document database.

        Args:
            model (str): The model ID for the AWS Bedrock model.
            temperature (float): Level of randomness for response generation.
            max_tokens (int): Maximum number of tokens in the generated response.
            docs_directory (str): Directory path to the documents database.
            chunk_size (int): Size of the chunks when splitting documents for retrieval.
            chunk_overlap (int): Size of the overlap between document chunks.
        """
        # Avoid multiple initializations
        if hasattr(self, '_initialized') and self._initialized:
            return
        self._initialized = True

        # Model parameter configuration
        self.model = model
        self.temperature = temperature
        self.max_tokens = max_tokens

        # Load prompt template, documents, and AWS client
        self.load_prompt_template()
        self.tfidf_retriever = utils.load_documents_database(docs_directory, chunk_size, chunk_overlap)
        self.aws_client = get_client()
        
        logger.info('AWS Bedrock Handler creado correctamente.')

    def load_prompt_template(self):
        """
        Loads the prompt template for generating queries.
        """
        try:
            self.prompt_template = (
                """
                Eres un asistente de IA que responde preguntas basadas en documentos.
                Solo responde en lenguaje markdown usando esos documentos. Si la información no está disponible, invita a visitar https://planestic.udistrital.edu.co/ o solicitar un ticket en https://mesadeayuda.planestic.udistrital.edu.co/.
                Pregunta: {question}
                Documentos: {context}
                """
            )
            self.prompt = PromptTemplate(
                template=self.prompt_template,
                input_variables=["context", "question"]
            )
            logger.info('PromptTemplate cargado correctamente.')
        except Exception as e:
            logger.error('Ha ocurrido un error al cargar el PromptTemplate.', exc_info=True)

    def get_answer(self, query: str) -> str:
        """
        Generates an answer for the given query using the AWS Bedrock model.

        Args:
            query (str): The user's query or question.

        Returns:
            str: The response generated by the AWS Bedrock model.
        """
        try:
            # Retrieve the most relevant documents
            retrieved_docs = self.tfidf_retriever.invoke(query)
            context = " ".join(doc.page_content for doc in retrieved_docs)

            # Build the conversation structure for the AWS Bedrock API
            conversation = [
                {
                    "role": "user",
                    "content": [{"text": self.prompt_template.format(
                        context=context,
                        question=query
                    )}]
                }
            ]

            # Call the AWS Bedrock model to get the response
            response = self.aws_client.converse(
                modelId=self.model,
                messages=conversation,
                inferenceConfig={
                    "maxTokens": self.max_tokens,
                    "temperature": self.temperature
                },
                additionalModelRequestFields={"k": 0}
            )

            # Extract and return the response generated by the model
            response_text = response["output"]["message"]["content"][0]["text"]
            return response_text

        except Exception as e:
            logger.error('Ha ocurrido un error al realizar la pregunta.', exc_info=True)
            return "Lo siento, ha ocurrido un error al procesar tu pregunta."
